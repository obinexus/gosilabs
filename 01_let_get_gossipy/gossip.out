#ifndef TOKEN_H
#define TOKEN_H

#include <stdio.h>
#include <stdlib.h>
#include <string.h>

// OBINexus Gosilang Token Definitions
// Medical device compliant token system
// #hacc #noghosting #sorrynotsorry

// Token Types - Complete Gosilang Grammar
typedef enum {
    // Operators & Delimiters
    TOKEN_BANG = 256,      // !
    TOKEN_HASH,            // #
    TOKEN_ASSIGN,          // :=
    TOKEN_EQUAL,           // =
    TOKEN_ARROW,           // ->
    TOKEN_LPAREN,          // (
    TOKEN_RPAREN,          // )
    TOKEN_LT,              // <
    TOKEN_GT,              // >
    TOKEN_LBRACKET,        // [
    TOKEN_RBRACKET,        // ]
    TOKEN_LBRACE,          // {
    TOKEN_RBRACE,          // }
    TOKEN_COMMA,           // ,
    TOKEN_COLON,           // :
    TOKEN_SEMICOLON,       // ;
    TOKEN_DOT_DOT,         // ..
    
    // Keywords
    TOKEN_DEF,             // #def
    TOKEN_BIND,            // #bind
    TOKEN_UNBIND,          // #unbind
    TOKEN_SPAN,            // span
    TOKEN_RANGE,           // range
    TOKEN_VEC,             // vec
    TOKEN_NIL,             // nil
    TOKEN_NULL,            // null
    TOKEN_LET,             // let
    
    // Literals & Identifiers
    TOKEN_IDENTIFIER,      // [A-Za-z_][A-Za-z0-9_]*
    TOKEN_INTEGER,         // [0-9]+
    TOKEN_FLOAT,           // [0-9]+\.[0-9]+
    
    // Special
    TOKEN_EOF,
    TOKEN_UNKNOWN,
    TOKEN_NEWLINE
} TokenType;

// Position tracking for medical device compliance
typedef struct {
    int line;
    int column;
    int offset;
} Position;

// Token structure with position and value
typedef struct {
    TokenType type;
    char *lexeme;
    Position pos;
    union {
        int int_val;
        double float_val;
    } value;
} Token;

// Token list for pipeline processing
typedef struct {
    Token *tokens;
    size_t count;
    size_t capacity;
} TokenList;

// YYSTYPE for flex/bison integration
typedef union {
    int num;
    double float_num;
    char *str;
} YYSTYPE;

extern YYSTYPE yylval;

// Function declarations
const char* token_type_name(TokenType type);
Token create_token(TokenType type, const char *lexeme, Position pos);
void token_list_init(TokenList *list);
void token_list_add(TokenList *list, Token token);
void token_list_free(TokenList *list);
void print_token_json(const Token *token);
void print_token_table(const Token *token);

// External declarations for lexer integration
extern TokenList global_tokens;
extern int lex_and_store();

#endif // TOKEN_H
#define _GNU_SOURCE
#define _DEFAULT_SOURCE
#define _GNU_SOURCE
#define _DEFAULT_SOURCE
#include "token.h"
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

extern FILE *yyin;
extern TokenList global_tokens;
extern int lex_and_store();

// ===== TOKEN UTILITIES =====
const char* token_type_name(TokenType type) {
    switch(type) {
        case TOKEN_BANG: return "BANG";
        case TOKEN_HASH: return "HASH";
        case TOKEN_ASSIGN: return "ASSIGN";
        case TOKEN_EQUAL: return "EQUAL";
        case TOKEN_ARROW: return "ARROW";
        case TOKEN_LPAREN: return "LPAREN";
        case TOKEN_RPAREN: return "RPAREN";
        case TOKEN_LT: return "LT";
        case TOKEN_GT: return "GT";
        case TOKEN_LBRACKET: return "LBRACKET";
        case TOKEN_RBRACKET: return "RBRACKET";
        case TOKEN_LBRACE: return "LBRACE";
        case TOKEN_RBRACE: return "RBRACE";
        case TOKEN_COMMA: return "COMMA";
        case TOKEN_COLON: return "COLON";
        case TOKEN_SEMICOLON: return "SEMICOLON";
        case TOKEN_DOT_DOT: return "DOT_DOT";
        case TOKEN_DEF: return "DEF";
        case TOKEN_BIND: return "BIND";
        case TOKEN_UNBIND: return "UNBIND";
        case TOKEN_SPAN: return "SPAN";
        case TOKEN_RANGE: return "RANGE";
        case TOKEN_VEC: return "VEC";
        case TOKEN_NIL: return "NIL";
        case TOKEN_NULL: return "NULL";
        case TOKEN_LET: return "LET";
        case TOKEN_IDENTIFIER: return "IDENTIFIER";
        case TOKEN_INTEGER: return "INTEGER";
        case TOKEN_FLOAT: return "FLOAT";
        case TOKEN_NEWLINE: return "NEWLINE";
        case TOKEN_EOF: return "EOF";
        case TOKEN_UNKNOWN: return "UNKNOWN";
        default: return "INVALID";
    }
}

Token create_token(TokenType type, const char *lexeme, Position pos) {
    Token token;
    token.type = type;
    token.lexeme = strdup(lexeme ? lexeme : "");
    token.pos = pos;
    token.value.int_val = 0; // default
    return token;
}

void token_list_init(TokenList *list) {
    list->tokens = malloc(sizeof(Token) * 32);
    list->count = 0;
    list->capacity = 32;
}

void token_list_add(TokenList *list, Token token) {
    if (list->count >= list->capacity) {
        list->capacity *= 2;
        list->tokens = realloc(list->tokens, sizeof(Token) * list->capacity);
    }
    list->tokens[list->count++] = token;
}

void token_list_free(TokenList *list) {
    for (size_t i = 0; i < list->count; i++) {
        free(list->tokens[i].lexeme);
    }
    free(list->tokens);
    list->tokens = NULL;
    list->count = 0;
    list->capacity = 0;
}

void print_token_table(const Token *token) {
    printf("| %-12s | %-15s | %4d:%-2d |\n", 
           token_type_name(token->type),
           token->lexeme,
           token->pos.line, token->pos.column);
}

// ===== PIPELINE STAGES =====

void stage1_raw_lexemes(const char *filename) {
    printf("\n=== STAGE 1: Raw Lexemes ===\n");
    FILE *file = fopen(filename, "r");
    if (!file) {
        perror("Cannot open file");
        return;
    }
    
    printf("Raw file content:\n");
    printf("─────────────────\n");
    char buffer[1024];
    while (fgets(buffer, sizeof(buffer), file)) {
        printf("%s", buffer);
    }
    printf("─────────────────\n");
    fclose(file);
}

void stage2_token_stream(const char *filename) {
    printf("\n=== STAGE 2: Token Stream ===\n");
    
    yyin = fopen(filename, "r");
    if (!yyin) {
        perror("Cannot open file");
        return;
    }
    
    int token_count = lex_and_store();
    fclose(yyin);
    
    printf("Generated %d tokens:\n\n", token_count);
    
    // Table format
    printf("Token Table:\n");
    printf("┌─────────────┬─────────────────┬─────────┐\n");
    printf("│ Token Type  │ Lexeme          │ Pos     │\n");
    printf("├─────────────┼─────────────────┼─────────┤\n");
    
    for (size_t i = 0; i < global_tokens.count; i++) {
        print_token_table(&global_tokens.tokens[i]);
    }
    printf("└─────────────┴─────────────────┴─────────┘\n");
}

void stage3_ast_preview() {
    printf("\n=== STAGE 3: AST Preview ===\n");
    printf("(Parser will build AST nodes from token stream)\n\n");
    
    // Mock AST analysis of token patterns
    printf("Detected patterns:\n");
    for (size_t i = 0; i < global_tokens.count; i++) {
        Token *token = &global_tokens.tokens[i];
        
        if (token->type == TOKEN_BANG) {
            printf("→ Invocation pattern starting at %d:%d\n", 
                   token->pos.line, token->pos.column);
        }
        if (token->type == TOKEN_BIND || token->type == TOKEN_UNBIND) {
            printf("→ Bind operation at %d:%d\n", 
                   token->pos.line, token->pos.column);
        }
        if (token->type == TOKEN_VEC) {
            printf("→ Vector construction at %d:%d\n", 
                   token->pos.line, token->pos.column);
        }
    }
}

void stage4_codegen_preview() {
    printf("\n=== STAGE 4: Codegen Preview ===\n");
    printf("(Will generate C skeleton from AST)\n\n");
    
    printf("Expected C output patterns:\n");
    printf("• #bind() → parallel_diff() calls\n");
    printf("• !vec<N>() → vec_make() + norm() calls\n");
    printf("• span[..] → normalize_to_span() calls\n");
    printf("• NIL handling → NaN or NIL_PTR checks\n");
}

// ===== MAIN PIPELINE =====

int main(int argc, char **argv) {
    if (argc < 2) {
        fprintf(stderr, "Usage: %s <file.gs> [--tokens|--raw|--all]\n", argv[0]);
        return 1;
    }
    
    const char *filename = argv[1];
    const char *output_mode = (argc > 2) ? argv[2] : "--all";
    
    printf("🔧 Gosilang MVP Lexer Pipeline\n");
    printf("📁 Processing: %s\n", filename);
    printf("🎯 OBINexus Computing - Services from the Heart ❤️\n");
    
    if (strcmp(output_mode, "--all") == 0) {
        stage1_raw_lexemes(filename);
        stage2_token_stream(filename);
        stage3_ast_preview();
        stage4_codegen_preview();
    } else if (strcmp(output_mode, "--tokens") == 0) {
        stage2_token_stream(filename);
    } else if (strcmp(output_mode, "--raw") == 0) {
        stage1_raw_lexemes(filename);
    }
    
    // Cleanup
    token_list_free(&global_tokens);
    
    printf("\n✅ Pipeline complete - ready for Phase 2 (Parser)\n");
    printf("#hacc #noghosting #sorrynotsorry\n");
    
    return 0;
}


## token.h - Complete Token Definitions
```c
#ifndef TOKEN_H
#define TOKEN_H

#include <stdio.h>
#include <stdlib.h>
#include <string.h>

// Token Types - Complete Gosilang Grammar
typedef enum {
    // Operators & Delimiters
    TOKEN_BANG = 256,      // !
    TOKEN_HASH,            // #
    TOKEN_ASSIGN,          // :=
    TOKEN_EQUAL,           // =
    TOKEN_ARROW,           // ->
    TOKEN_LPAREN,          // (
    TOKEN_RPAREN,          // )
    TOKEN_LT,              // <
    TOKEN_GT,              // >
    TOKEN_LBRACKET,        // [
    TOKEN_RBRACKET,        // ]
    TOKEN_LBRACE,          // {
    TOKEN_RBRACE,          // }
    TOKEN_COMMA,           // ,
    TOKEN_COLON,           // :
    TOKEN_SEMICOLON,       // ;
    TOKEN_DOT_DOT,         // ..
    
    // Keywords
    TOKEN_DEF,             // #def
    TOKEN_BIND,            // #bind
    TOKEN_UNBIND,          // #unbind
    TOKEN_SPAN,            // span
    TOKEN_RANGE,           // range
    TOKEN_VEC,             // vec
    TOKEN_NIL,             // nil
    TOKEN_NULL,            // null
    TOKEN_LET,             // let
    
    // Literals & Identifiers
    TOKEN_IDENTIFIER,      // [A-Za-z_][A-Za-z0-9_]*
    TOKEN_INTEGER,         // [0-9]+
    TOKEN_FLOAT,           // [0-9]+\.[0-9]+
    
    // Special
    TOKEN_EOF,
    TOKEN_UNKNOWN,
    TOKEN_NEWLINE
} TokenType;

// Position tracking
typedef struct {
    int line;
    int column;
    int offset;
} Position;

// Token structure with position
typedef struct {
    TokenType type;
    char *lexeme;
    Position pos;
    union {
        int int_val;
        double float_val;
    } value;
} Token;

// Token list for pipeline
typedef struct {
    Token *tokens;
    size_t count;
    size_t capacity;
} TokenList;

// Function declarations
const char* token_type_name(TokenType type);
Token create_token(TokenType type, const char *lexeme, Position pos);
void token_list_init(TokenList *list);
void token_list_add(TokenList *list, Token token);
void token_list_free(TokenList *list);
void print_token_json(const Token *token);
void print_token_table(const Token *token);

#endif // TOKEN_H
```

## lexer.l - Complete Flex Specification
```lex
%{
#include "token.h"
#include <stdlib.h>
#include <string.h>

// Global position tracking
Position current_pos = {1, 1, 0};
TokenList global_tokens;

void update_position() {
    for (int i = 0; yytext[i]; i++) {
        if (yytext[i] == '\n') {
            current_pos.line++;
            current_pos.column = 1;
        } else {
            current_pos.column++;
        }
        current_pos.offset++;
    }
}

Token make_token(TokenType type) {
    Position start_pos = current_pos;
    Token token = create_token(type, yytext, start_pos);
    update_position();
    return token;
}

Token make_int_token() {
    Token token = make_token(TOKEN_INTEGER);
    token.value.int_val = atoi(yytext);
    return token;
}

Token make_float_token() {
    Token token = make_token(TOKEN_FLOAT);
    token.value.float_val = atof(yytext);
    return token;
}
%}

%option noyywrap
%option yylineno

/* Pattern Definitions */
DIGIT       [0-9]
ALPHA       [A-Za-z_]
ALNUM       [A-Za-z0-9_]
WHITESPACE  [ \t\r]
NEWLINE     \n

%%

    /* Compound Operators (match first to avoid conflicts) */
":="            { return make_token(TOKEN_ASSIGN).type; }
"->"            { return make_token(TOKEN_ARROW).type; }
".."            { return make_token(TOKEN_DOT_DOT).type; }

    /* Keywords - Must come before identifiers */
"#def"          { return make_token(TOKEN_DEF).type; }
"#bind"         { return make_token(TOKEN_BIND).type; }
"#unbind"       { return make_token(TOKEN_UNBIND).type; }
"span"          { return make_token(TOKEN_SPAN).type; }
"range"         { return make_token(TOKEN_RANGE).type; }
"vec"           { return make_token(TOKEN_VEC).type; }
"nil"           { return make_token(TOKEN_NIL).type; }
"null"          { return make_token(TOKEN_NULL).type; }
"let"           { return make_token(TOKEN_LET).type; }

    /* Single Character Operators */
"!"             { return make_token(TOKEN_BANG).type; }
"#"             { return make_token(TOKEN_HASH).type; }
"="             { return make_token(TOKEN_EQUAL).type; }
"("             { return make_token(TOKEN_LPAREN).type; }
")"             { return make_token(TOKEN_RPAREN).type; }
"<"             { return make_token(TOKEN_LT).type; }
">"             { return make_token(TOKEN_GT).type; }
"["             { return make_token(TOKEN_LBRACKET).type; }
"]"             { return make_token(TOKEN_RBRACKET).type; }
"{"             { return make_token(TOKEN_LBRACE).type; }
"}"             { return make_token(TOKEN_RBRACE).type; }
","             { return make_token(TOKEN_COMMA).type; }
":"             { return make_token(TOKEN_COLON).type; }
";"             { return make_token(TOKEN_SEMICOLON).type; }

    /* Numbers */
{DIGIT}+\.{DIGIT}+  { return make_float_token().type; }
{DIGIT}+            { return make_int_token().type; }

    /* Identifiers */
{ALPHA}{ALNUM}*     { return make_token(TOKEN_IDENTIFIER).type; }

    /* Whitespace */
{WHITESPACE}+       { update_position(); /* skip */ }
{NEWLINE}           { return make_token(TOKEN_NEWLINE).type; }

    /* Unknown characters */
.                   { return make_token(TOKEN_UNKNOWN).type; }

%%

/* Store tokens for pipeline access */
int lex_and_store() {
    int token_type;
    token_list_init(&global_tokens);
    
    while ((token_type = yylex()) != 0) {
        // Token already stored via make_token calls
        Token token = create_token(token_type, yytext, current_pos);
        token_list_add(&global_tokens, token);
        
        if (token_type == TOKEN_EOF) break;
    }
    
    return global_tokens.count;
}
